<!DOCTYPE html>
<html>
<head>

  <meta charset="UTF-8">
  <title>Implementing Logistic Regression</title>
  <meta name="viewport" content="width=device-width">

  <!--[if lt IE 9]>
    <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->

  <link href="/assets/css/style.css" rel="stylesheet" />
  <link href="/assets/css/colors-dark.css" rel="stylesheet" />

</head>

<body>



  <header id="header">
    <h1><img src="/images/icons8-plus-1-hour-100.png" height="40px" width="40px" style="vertical-align: middle;"> Phillip Williams</h1>
    <p>An Hour a Day</p>
  </header>



  <div id="page">



    <div id="sidebar">
      <nav>
        <ul>
          <li><a href="/">Home</a></li>
          <li><a href="/about">About</a></li>
          <li><a href="/projects">Projects</a></li>
          <li><a href="/publications">Publications</a></li>
        </ul>
      </nav>
    </div>



    <div id="content">
      <article class="post">

	
		<h1><a href="/2019/01/23/Implementing-Logistic-Regression/">Implementing Logistic Regression</a></h1>
	

	<div class="post-content"><p>One of the simplest Machine Learning algorithms is Logistic Regression. At a conceptual level, there’s not much more to it than some simple calculus, but this algorithm can still be pretty effective in a lot of situations. In this post, we’re going to take a little bit of a look at the math behind Logistic Regression and then implement our own Logistic Regression library in python.</p>

<h3 id="what-is-logistic-regression">What is Logistic Regression?</h3>

<p>First of all, when we talk about Machine Learning, we are really talking about curve fitting. What this means is that we have some numerical input data as well as the numerical output we want, we’ll then use that data to create a mathmatical model that can take in some input data and output the correct values.</p>

<p>In the case of Logistic Regression, we take in a vector of numerical values, and we get an output between 0 and 1. This is a specific type of Machine Learning classification. Basically, we want to know if something about the input data is true or false, with 1 corresponding to true and 0 corresponding to false.</p>

<p>Now that we’ve got that, what is Logistic Regression really? Logistic Regression is defined by two main equations:</p>

<script type="math/tex; mode=display">z = \sum{w_i \cdot x_i}</script>

<p>and</p>

<script type="math/tex; mode=display">y(z) = \frac{1}{1+e^{-z}}</script>

<p>$ x_i $ is the ith element of our input vector, $ w_i $ is the weight of that specific input and $ z $ is the weighted sum of the $ x $ and $ w $ vectors. $ y(z) $ on the other hand, is the final output of the Logistic Regression equation and looks like this:</p>

<canvas id="line-chart" width="800" height="450"></canvas>
<script>
document.addEventListener("DOMContentLoaded", function(){

	var x = [];

	for(var i = 0; i < 41; i++){
		x.push( (i / 4) - 5);
	}


	new Chart(document.getElementById("line-chart"), {
	  type: 'line',
	  data: {
	    labels: x,
	    datasets: [{ 
	        data: x.map(i => 1/(1+Math.exp(-i))),
	        label: "y(z)",
	        borderColor: "#8e5ea2",
	        fill: false
	      }
	    ]
	  },
	  options: {
	  	legend :{
	  		display: false
	  	},
	  	tooltips :{
	  		enabled: false
	  	},
	    title: {
	      display: true,
	      text: 'Logistic Function',
	      fontColor: "#839496"
	    },
	    scales: {
		    yAxes: [{
		      gridLines: {
			    color: "#839496"
			  },
		      ticks :{
		      	fontColor: "#839496"
		      },
		      scaleLabel: {
		        display: true,
		        labelString: 'y(z)',
		        fontColor: "#839496"
		      }
		    }],
		  xAxes: [{
		  	  gridLines: {
			    color: "#839496"
			  },
		  	  ticks :{
		      	fontColor: "#839496"
		      },
		      scaleLabel: {
		        display: true,
		        labelString: 'z',
		        fontColor: "#839496"
		      }
		    }]
		}     
	  }
	});
	
});
</script>

<h3 id="training-a-linear-model">Training a Linear Model</h3>

<p>So now we have an idea of what our model looks like and how it is defined. The next step is to actually train the model by solving for the $ w $ vector. Assuming we have a dataset of $ x $ vectors (all of the same size) and $ y $ values that we want to predict, we want to find our weight vector $ w $ that will maximize the accuracy of our model and give correct predictions.</p>

<p>The are several algorithms that can do this, each having their own pros and cons, such as <a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a> or Genetic Algorithms. However, we are going to train our Logistic Regression model using nothing but Linear Regression.</p>

<p>Linear Regression lets us fit a simple linear model defined by the following equation:</p>

<script type="math/tex; mode=display">y = \sum{x_i \cdot b_i}</script>

<p>$ b $ is our weight vector for the Linear Model and is obtained by the Ordinay Least Squares:</p>

<script type="math/tex; mode=display">B = (X^{T}X)^{-1})(X^{T}Y)</script>

<p>When solving for $ B $, $ X $ is a 2D matrix, each row corresponds to a single input vector, $ Y $ is a vector of the desired outputs for each input vector and $X^T$ and $X^-1$ are the matrix operations of transposing and inverting respectively. We will not implement these matrix functions ourselves, but will instead use the built in NumPy functions for ease.</p>

<p>This should seem very similar, since it is exactly the same equation for $ z $ in the Logistic Regression model, the only difference is that we pass the sum through a non-linear transformation in Logistic Regression.</p>

<h3 id="bridging-linear-and-logistic-regression">Bridging Linear and Logistic Regression</h3>

<p>This is where we can use a clever trick to transform the Logistic Regression problem into a Linear Regression problem. By applying the following function to the true/false (1/0) values of the classification, we can get equivalent values to train a Linear Regression model :</p>

<script type="math/tex; mode=display">z = ln( \frac{y}{1-y})</script>

<p>However, if we plug in the values of 0 and 1, we will get a domain error since we can’t divide by 0 or calculate the log of 0. To solve this, we can simply use values arbitrarily close to 0 and 1 for our classification output, for example 0.001 and 0.999. This technique is called Label Smoothing.</p>

<p>So now, to train our Logistic Regression model, we take the classification output of 1 or 0, add some small constant to avoid numerical errors, train a Linear Regression model on the transformed data, then use the Linear Model and the Logistic function to make predictions on new data.</p>

<h3 id="recap-of-the-algorithm">Recap of the algorithm</h3>

<p>Here is a recap of the algorithm to implement Logistic Regression, assuming you have a collection of numerical input vectors and the desired true/false output label:</p>
<ol>
  <li>Use label smoothing to convert each 0/1 label into 0.001/0.999 to avoid numerical issues.</li>
  <li>Convert the smoothed labels into the linear domain using the following equation, where $ y $ is the smoothed label and $ z $ is the linear value:
    <ul>
      <li>
        <script type="math/tex; mode=display">z = ln( \frac{y}{1-y})</script>
      </li>
    </ul>
  </li>
  <li>Solve for the weight vector $ B $ using the following equation:
    <ul>
      <li>
        <script type="math/tex; mode=display">B = (X^{T}X)^{-1}(X^{T}Y)</script>
      </li>
    </ul>
  </li>
  <li>Use the weight vector $ B $ and a new input vector $ x $ to predict the output for this unkown vector, $ y(z) $ is the predicted output:
    <ul>
      <li>
        <script type="math/tex; mode=display">z = \sum{w_i \cdot x_i}</script>
      </li>
      <li>
        <script type="math/tex; mode=display">y(z) = \frac{1}{1+e^{-z}}</script>
      </li>
    </ul>
  </li>
</ol>

<p>Now that all the of the theoretical equations have been established, we can actually implement our model and test it on some real world data. For this example, we will be using the <a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)">UCI ML Breast Cancer Wisconsin (Diagnostic) dataset</a>. You can download a copy of the dataset directly, or you can import it through the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html">Scikit learn dataset module</a>. We are trying to predict if a tumor is bening or malignant with several features such as the radius, symmetry, smoothness and texture.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">math</span>


<span class="c"># Define class to implement our logistic regression model</span>
<span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">():</span>

	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
		<span class="c"># bias determines if we use a bias term or not</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
		<span class="c"># instance variable for our weight vector</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
		<span class="c"># the cutoff is used to determine the prediction, if y(z) &gt;= cutoff, y(z) = 1, else y(z) = 0</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span> <span class="o">=</span> <span class="n">cutoff</span>
		<span class="c"># the amount of smoothing used on the output labels</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">smooth</span> <span class="o">=</span> <span class="n">smooth</span>

	<span class="c"># will smooth all the labels given the Y vector</span>
	<span class="k">def</span> <span class="nf">smooth_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>

		<span class="n">y_smooth</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)):</span>
			<span class="k">if</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="p">:</span>
				<span class="n">y_smooth</span> <span class="o">+=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="p">]</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">y_smooth</span> <span class="o">+=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="p">]</span>

		<span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_smooth</span><span class="p">)</span>

	<span class="c"># calculates y given a single z</span>
	<span class="k">def</span> <span class="nf">logistic_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
		<span class="n">y</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">y</span>

	<span class="c"># calculates z given a single y</span>
	<span class="k">def</span> <span class="nf">inverse_logistic_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
		<span class="n">z</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">z</span>

	<span class="c"># convert the labels from 0/1 values to linear values</span>
	<span class="k">def</span> <span class="nf">transform_labels_to_linear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)):</span>
			<span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse_logistic_function</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
		<span class="k">return</span> <span class="n">Y</span>

	<span class="c"># use the weights and a new vector to make a prediction</span>
	<span class="k">def</span> <span class="nf">predict_on_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

		<span class="c"># calculate the weighted sum</span>
		<span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>

		<span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logistic_function</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

		<span class="k">if</span> <span class="n">prediction</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span><span class="p">:</span>
			<span class="k">return</span> <span class="mi">1</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="k">return</span> <span class="mi">0</span>

	<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>

		<span class="c"># using a bias will add a feature to each vector that is set to 1</span>
		<span class="c"># this allows the model to learn a "default" value from this constant</span>
		<span class="c"># the bias can be thought of as the offset, while the weights are the slopes</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
			<span class="n">ones</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))])</span>
			<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

		<span class="c"># calculate the prediction for each vector</span>
		<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>

			<span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_on_vector</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
			<span class="n">predictions</span> <span class="o">+=</span> <span class="p">[</span><span class="n">prediction</span><span class="p">]</span>

		<span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>


	<span class="c"># train the model on the dataset</span>
	<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>

		<span class="c"># using a bias will add a feature to each vector that is set to 1</span>
		<span class="c"># this allows the model to learn a "default" value from this constant</span>
		<span class="c"># the bias can be thought of as the offset, while the weights are the slopes</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
			<span class="n">ones</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))])</span>
			<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

		<span class="c"># smooth the labels</span>
		<span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth_labels</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
		<span class="c"># convert labels to linear</span>
		<span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform_labels_to_linear</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
		<span class="c"># calculate weights</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
				<span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span>
					<span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
						<span class="n">numpy</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
						<span class="n">X</span>
					<span class="p">)</span>
				<span class="p">),</span>
				<span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
					<span class="n">numpy</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
					<span class="n">Y</span>
				<span class="p">)</span>
			<span class="p">)</span>

<span class="c"># Apply the logistic regression model to the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c">#split the data into training and testing sets</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">100</span><span class="p">:]</span>

<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="mi">100</span><span class="p">:]</span>

<span class="c"># Initialize the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c"># Train the model on the training set</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="c"># calculate the accuracy on the training set</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Training Accuracy (</span><span class="si">%</span><span class="s">) = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">Y_train</span><span class="p">)</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_train</span><span class="p">))))</span>

<span class="c"># calculate the accuracy on the testing set</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Testing Accuracy (</span><span class="si">%</span><span class="s">) = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">Y_test</span><span class="p">)</span><span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))))</span></code></pre></figure>

</div>

	<p class="meta">Posted on <span class="postdate">Jan 23, 2019</span></p>

	
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>

<script src="/scripts/chart.js">
</script>


</article>
    </div>



  </div>



  <footer id="footer">
    <p class="copyright">Copyright &copy; 2019 Phillip Williams. Powered by <a href="http://jekyllrb.com">Jekyll</a>, theme by <a href="http://www.webmaster-source.com">Matt Harzewski</a></p>
  </footer>



  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.1/jquery.min.js"></script>
  <script src="/assets/js/jquery.mobilemenu.min.js"></script>

  <script>
    $(document).ready(function(){
      $('#sidebar nav ul').mobileMenu({'topOptionText': 'Menu', 'prependTo': '#sidebar nav'});
    });
  </script>



</body>
</html>